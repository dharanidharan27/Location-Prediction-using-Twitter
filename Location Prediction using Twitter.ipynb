{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import spacy\n",
    "import tweepy\n",
    "import datetime\n",
    "import xlsxwriter\n",
    "import wikipedia\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from geotext import GeoText\n",
    "from nltk.corpus import stopwords    \n",
    "from nltk.tokenize import word_tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "citylist=[\"Bradford\" ,\"London\" ,\"Manchester\"]\n",
    "final_dataframe4 = pd.DataFrame(columns=citylist)\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "s2 = \"See also\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(citylist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def parse_words_first_level():\n",
    "city_word=[]\n",
    "allcity_word=[]\n",
    "city_wikicontent=[]\n",
    "for city in citylist:\n",
    "    words_refined_citywise=[]\n",
    "    print(\"**********************************************************************\")\n",
    "    print(\"taking content for\", city)\n",
    "    p=wikipedia.page(city)\n",
    "    city_wikicontent = p.content\n",
    "    s1 = []\n",
    "    string = (city_wikicontent[:city_wikicontent.index(s2)])\n",
    "    print(string)\n",
    "    doc = nlp(string)\n",
    "    print(\"$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\")\n",
    "    print(\"taking each city words thro nlp\")\n",
    "    eachcity_word=[]\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ not in {'QUANTITY', 'DATE', 'ORDINAL', 'CARDINAL', 'PERCENT', 'MONEY'}:\n",
    "            if ent.text != '\\n' and len(ent.text)>2:\n",
    "                print(\"appending words\" , ent.text)\n",
    "                eachcity_word.append(ent.text)\n",
    "    city_unique_words=set(eachcity_word)\n",
    "    words_refined_citywise=list(city_unique_words)\n",
    "    print(\"length of \",city,\" is\", len(words_refined_citywise))\n",
    "    print(words_refined_citywise)\n",
    "    print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "    allcity_word.append((city,words_refined_citywise))\n",
    "    print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "    print(\"^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\")\n",
    "    print(\"length of all city list is\", len(allcity_word))\n",
    "    print(allcity_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allcity_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G1 = nx.DiGraph()\n",
    "G2 = nx.DiGraph()\n",
    "G3 = nx.DiGraph()\n",
    "G = [G1,G2,G3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_word = zip(G,citylist,allcity_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_words=list(combine_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "counter_list=[]\n",
    "complete_pack=[]\n",
    "eachcity_word2=[]\n",
    "city_words_links=[]\n",
    "cities_link_content=[]\n",
    "each_city_link=[]\n",
    "for x in combine_words:\n",
    "    citylinks = (wikipedia.page(x[count+1])).links\n",
    "    each_city_link.append((x[count+1],citylinks))\n",
    "    obj=x[0]\n",
    "    obj.add_node(x[count+1])\n",
    "    #print(x[count+1])\n",
    "    for y in citylinks:\n",
    "        city_link_content=[]\n",
    "        print(y)\n",
    "        obj.add_node(y)\n",
    "        obj.add_edge(x[count+1],y)\n",
    "        words_wiki = wikipedia.page(y)\n",
    "        wiki_cont = words_wiki.content\n",
    "        words_wikilinks = words_wiki.links\n",
    "        s1 = []\n",
    "        doc = []\n",
    "        counter = 0\n",
    "        # i think i can do removal of stop words first\n",
    "        if s2 in wiki_cont:\n",
    "            string = (wiki_cont[:wiki_cont.index(s2)])\n",
    "            doc = nlp(string)\n",
    "        else:\n",
    "            doc = nlp(wiki_cont)\n",
    "        for ent in doc.ents:\n",
    "                if ent.label_ not in {'QUANTITY', 'DATE', 'ORDINAL', 'CARDINAL', 'PERCENT', 'MONEY'}:\n",
    "                    if ent.text != '\\n' and len(ent.text)>2:\n",
    "                        city_link_content.append(ent.text)\n",
    "        removing_duplicates = list(set(city_link_content))\n",
    "        cities_link_content.append(removing_duplicates)\n",
    "        #---- implement -- cross check with all cities and see how much they match\n",
    "        for h in removing_duplicates:\n",
    "            word = h\n",
    "            if word in x[2][1]:\n",
    "                counter+=1\n",
    "        counter_list.append((x[1],y,counter,len(x[2][1]),len(city_link_content)))\n",
    "        for z in words_wikilinks:\n",
    "            if z in citylinks:\n",
    "                obj.add_edge(y,x[count+1])\n",
    "            else:\n",
    "                obj.add_node(z)\n",
    "                obj.add_edge(y,z)\n",
    "        city_words_links.append((x[1],y,removing_duplicates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "each_city_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw_networkx(G1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_words_links[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pagerank_cities=[]\n",
    "for graph in G:\n",
    "    pr = nx.pagerank(graph, alpha=0.9)\n",
    "    pagerank_cities.append((pr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pagerank_cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " counter_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity measures by overlapping:\n",
    "similarity_overlaping=[]\n",
    "for g in counter_list:\n",
    "    static = g[2]\n",
    "    similarity = static/(g[3]+g[4])\n",
    "    similarity_overlaping.append((g[0],g[1],similarity))\n",
    "print(similarity_overlaping)\n",
    "\n",
    "df3 = pd.DataFrame(columns=['city', 'link', 'similarity'])\n",
    "for i in range (len(similarity_overlaping)):\n",
    "    df3.at[i, 'city'] = similarity_overlaping[i][0]\n",
    "    df3.at[i, 'link'] = similarity_overlaping[i][1]\n",
    "    df3.at[i, 'similarity'] = similarity_overlaping[i][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "each_city_link[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taking top 3 cities using overlapping scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_score_overlap(ip_words):\n",
    "    d1 = {}\n",
    "    d3={}\n",
    "    tweet = ip_words\n",
    "    calculating_score1=[]\n",
    "    calculating_score2=[]\n",
    "    calculating_score3=[]\n",
    "    for s in tweet:\n",
    "        print(s)\n",
    "        for o in city_words_links:\n",
    "            if s in o[2]:\n",
    "                score=float(df3.loc[df3['link'] == o[1], 'similarity'])\n",
    "                calculating_score1.append((o[0],score))\n",
    "            else:\n",
    "                calculating_score1.append((o[0],0))\n",
    "        for word in allcity_word:\n",
    "            #print(word[1])\n",
    "            if s in word[1]:\n",
    "                calculating_score2.append((word[0],0.5))\n",
    "            else:\n",
    "                calculating_score2.append((word[0],0))\n",
    "        for ci in each_city_link:\n",
    "            if s in ci[1]:\n",
    "                calculating_score3.append((ci[0],0.75))\n",
    "            else:\n",
    "                calculating_score3.append((ci[0],0))\n",
    "    for company, n in calculating_score1:\n",
    "        key = (company)\n",
    "        d1[key] = d1.get(key, 0) + n\n",
    "    #print(d1)\n",
    "    for company3, n3 in calculating_score3:\n",
    "        key3 = (company3)\n",
    "        d3[key3] = d3.get(key3, 0) + n3\n",
    "    #print(d3)\n",
    "    d2 = {}\n",
    "    for company2, n2 in calculating_score2:\n",
    "        key2 = (company2)\n",
    "        d2[key2] = d2.get(key2, 0) + n2\n",
    "    #print(d2)\n",
    "    final_score = { k: d1.get(k, 0) + d2.get(k, 0) + d2.get(k, 0)  for k in set(d1) | set(d2) | set(d3)}\n",
    "    print('final',final_score)\n",
    "    return final_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taking top 3 cities using betweeness centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = nx.algorithms.centrality.betweenness_centrality(G1,normalized=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Extracting more tweets of an user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handling rate limit errors\n",
    "def limit_handled(cursor):\n",
    "    while True:\n",
    "        try:\n",
    "            yield cursor.next()\n",
    "        except tweepy.RateLimitError:\n",
    "            print(\"Time Limit. Sleeping now....\")\n",
    "            time.sleep(15 * 60)\n",
    "            print(\"Trying again\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractTweets(screen_name):\n",
    "    #create empty data frame\n",
    "    dataFrame = pd.DataFrame()\n",
    "    tweet_count=0\n",
    "    retweet_count=0\n",
    "    fav_count=0\n",
    "    all_words =[]\n",
    "    tweet_text=[] # list of tweets\n",
    "    retweet_text=[] # list of retweets\n",
    "    fav_tweet_text=[] # list of favorite tweets\n",
    "    all_geo_text=[]\n",
    "    for tweets in limit_handled(tweepy.Cursor(api.user_timeline,screen_name=screen_name,count=200,tweet_mode='extended').items()):\n",
    "        #print(tweets)\n",
    "        #print(\"Created at:%s, tweet is : %s \" %(tweets.created_at,tweets.text))\n",
    "        if (tweets.lang == \"en\"):\n",
    "            raw_text = tweets.full_text\n",
    "            text = re.sub(r\"http\\S+\",\"\",raw_text)\n",
    "            geo_text = GeoText(removeNonAscii(text)).cities\n",
    "            all_geo_text+=geo_text\n",
    "            twt_text = (nlp(removeNonAscii(text)))\n",
    "            for ent in twt_text.ents:\n",
    "                #print('twt_text',ent)\n",
    "                if ent.label_ not in {'QUANTITY', 'DATE', 'ORDINAL', 'CARDINAL', 'PERCENT', 'MONEY'}:\n",
    "                    if ent.text != '\\n' and len(ent.text)>2:\n",
    "                        print(\"appending words\" , ent.text)\n",
    "                        tweet_text.append(ent.text)\n",
    "            tweet_count+=1\n",
    "        if hasattr(tweets,'retweeted_status'):\n",
    "            if (tweets.retweeted_status.lang == \"en\"):\n",
    "                raw_text = tweets.retweeted_status.full_text\n",
    "                text = re.sub(r\"http\\S+\",\"\",raw_text)\n",
    "                retwt_text = (nlp(removeNonAscii(text)))\n",
    "                geo_text1 = GeoText(removeNonAscii(text)).cities\n",
    "                all_geo_text+=geo_text1\n",
    "                for ent in retwt_text.ents:\n",
    "                    #print('retweet text',ent)\n",
    "                    if ent.label_ not in {'QUANTITY', 'DATE', 'ORDINAL', 'CARDINAL', 'PERCENT', 'MONEY'}:\n",
    "                        if ent.text != '\\n' and len(ent.text)>2:\n",
    "                            print(\"appending words\" , ent.text)\n",
    "                            retweet_text.append(ent.text)\n",
    "                retweet_count+=1        \n",
    "        if(tweet_count == 1000 or retweet_count == 1000):\n",
    "            break\n",
    "    for fav_tweets in limit_handled(tweepy.Cursor(api.favorites,id=screen_name,tweet_mode='extended').items()):\n",
    "        if(fav_tweets.lang  == 'en'):\n",
    "            raw_text = fav_tweets.full_text\n",
    "            text = re.sub(r\"http\\S+\",\"\",raw_text)\n",
    "            geo_text2 = GeoText(removeNonAscii(text)).cities\n",
    "            all_geo_text+=geo_text2\n",
    "            fav_text = (nlp(removeNonAscii(text)))\n",
    "            for ent in fav_text.ents:\n",
    "                #print('retweet text',ent)\n",
    "                if ent.label_ not in {'QUANTITY', 'DATE', 'ORDINAL', 'CARDINAL', 'PERCENT', 'MONEY'}:\n",
    "                    if ent.text != '\\n' and len(ent.text)>2:\n",
    "                        #print(\"appending words\" , ent.text)\n",
    "                        fav_tweet_text.append(ent.text)\n",
    "                        \n",
    "            fav_count+=1\n",
    "        if (fav_count > 50):\n",
    "            break\n",
    "    all_words = tweet_text+retweet_text+fav_tweet_text+all_geo_text\n",
    "    #print('all_words',all_words)\n",
    "    return all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def authentication():\n",
    "    auth = tweepy.OAuthHandler('gcVOEVJqRYvglu1k5KzsdEqMM','ikxcyneBliQZm0Ea8jDV7SX3myuLoctt6vjSDYhL2uDr5hYmqU')\n",
    "    auth.set_access_token('972247239824461826-tdmBijOQHifuQ5WOJBMkU9rUd8ThAMy','xHaN3RZz48JDHqnIQVxR15DdeHnb7OQEjQCtPZSWar250')\n",
    "    api=tweepy.API(auth,wait_on_rate_limit=True)\n",
    "    return api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = authentication()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeNonAscii(text):\n",
    "    return \"\".join(char for char in text if ord(char) < 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_excel('Brad.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_en = pd.DataFrame(df.loc[df['Tweet language (ISO 639-1)'] == 'en'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### change lists to df_en['User Name'] while submitting\n",
    "for ing in range(len(df_en[:1500])):\n",
    "    print(df.loc[df.index[ing],'Tweet Id'])\n",
    "    #words_From_prev_tweets = extractTweets(screenname)\n",
    "    #print('afterrrrrrrrrr extractTweets_applynlp')\n",
    "    words_bio_data=[]\n",
    "    geo_text_here=[]\n",
    "    bio=df.loc[df.index[ing],'Bio']\n",
    "    #print('biooooooooooo',bio)\n",
    "    id_of_tweet=df.loc[df.index[ing],'Tweet Id']\n",
    "    #print(id_of_tweet)\n",
    "    try:\n",
    "        tweetz = api.get_status(id_of_tweet)\n",
    "        tweet=tweetz.text\n",
    "    except:\n",
    "        print('tweet deleted')\n",
    "        tweet=df.loc[df.index[ing],'Tweet content']\n",
    "    text1 = re.sub(r\"http\\S+\",\"\",str(bio))\n",
    "    text2 = re.sub(r\"http\\S+\",\"\",str(tweet))\n",
    "    doc3 = nlp(removeNonAscii(text1))\n",
    "    doc4 = nlp(removeNonAscii(text2))    \n",
    "    #print('doc3',doc3, 'doc44',doc4)\n",
    "    geo_text_here+=GeoText(removeNonAscii(text1)).cities\n",
    "    geo_text_here+=GeoText(removeNonAscii(text2)).cities\n",
    "    #print('getting geo text both in tweet and bio')\n",
    "    #print(geo_text_here)\n",
    "    for ent in doc3.ents:\n",
    "        if ent.label_ not in {'QUANTITY', 'DATE', 'ORDINAL', 'CARDINAL', 'PERCENT', 'MONEY'}:\n",
    "            if ent.text != '\\n' and len(ent.text)>2:\n",
    "                words_bio_data.append(ent.text)\n",
    "    for ent in doc4.ents:\n",
    "        if ent.label_ not in {'QUANTITY', 'DATE', 'ORDINAL', 'CARDINAL', 'PERCENT', 'MONEY'}:\n",
    "            if ent.text != '\\n' and len(ent.text)>2:\n",
    "                words_bio_data.append(ent.text)\n",
    "    non_duplicate_words_here = list(set(words_bio_data))\n",
    "    print('non_duplicate_words_here',non_duplicate_words_here)\n",
    "    #print('words_From_prev_tweets',words_From_prev_tweets)\n",
    "    #print(list(set(geo_text_here)))\n",
    "    #final_list = non_duplicate_words_here + words_From_prev_tweets + list(set(geo_text_here))\n",
    "    final_list = non_duplicate_words_here + list(set(geo_text_here))\n",
    "\n",
    "    print('finallist for',final_list)\n",
    "    filldataframe = cal_score_overlap(final_list)\n",
    "    print(filldataframe)\n",
    "    final_dataframe4 = final_dataframe4.append(filldataframe,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat=df_en['Latitude']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long=df_en['Longitude']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataframe4['latitude'] = lat\n",
    "final_dataframe4['longitude'] = long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding Latitude\n",
    "datamodel = final_dataframe4.dropna()\n",
    "lat_values= datamodel['latitude']\n",
    "long_values=datamodel['longitude']\n",
    "datamodel_ip = datamodel.drop(['latitude','longitude'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(datamodel_ip, lat_values, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(datamodel_ip, long_values, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm \n",
    "model = svm.SVR(kernel='linear',C=1, gamma=1) \n",
    "# there is various option associated with it, like changing kernel, gamma and C value. Will discuss more # about it in next section.Train the model using the training sets and check score\n",
    "model.fit(X_train, y_train)\n",
    "predicted= model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = svm.SVR(kernel='linear',C=1, gamma=1) \n",
    "model1.fit(X_train1, y_train1)\n",
    "predicted1= model1.predict(X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_df = pd.DataFrame()\n",
    "accuracy_df['Actual Lat'] = y_test\n",
    "accuracy_df['Actal Long'] = y_test1\n",
    "accuracy_df['Predicted Lat'] = predicted\n",
    "accuracy_df['Predicted Long'] = predicted1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sin, cos, sqrt, atan2, radians\n",
    "over150=[]\n",
    "below150=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = list(zip(y_test,y_test1))\n",
    "list2 = list(zip(predicted,predicted1))\n",
    "for i in range(len(list1)):\n",
    "    lat1 = list1[i][0]\n",
    "    lon1 = list1[i][1]\n",
    "    lat2 = list2[i][0]\n",
    "    lon2 = list2[i][1]\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "    distance = R * c\n",
    "    if distance>150:\n",
    "        over150.append(distance)\n",
    "    else:\n",
    "        below150.append(distance)\n",
    "    print(\"Result:\", distance)\n",
    "print('Accuracy' ,(len(below150)/(len(over150)+len(below150)))*100,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
